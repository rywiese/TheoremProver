\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Writing Assignment 5}
\author{Arman Shah and Ry Wiese\\ shahx252@umn.edu  wiese176@umn.edu }
\date{December 2018}

\begin{document}

\maketitle

\section{Background}

There is plenty of research in area of generating proofs. Asperti in 2011 wrote about the interactive theorem prover \cite{asperti2011matita}. Essentially, the research helps the user create a proof. It is mainly used for verifying correctness of the users proof, but can also simplify what the user is writing. Another interesting area in the research is the product can correct for when the user makes a logical paradox while trying to prove something. The paper doesn't go into detail on how they accomplish this, but rather explain what they can do. Another problem is this relies heavily on a user to create a proof, while our problem is to automatically do that.

In 2007, Bonichon wrote about their automated theorem prover, Zenon, for first order logic \cite{bonichon2007zenon}. While testing, Zenon was shown to be able to find proofs for many problems. The problems that Zenon couldn't find proofs for were mainly because of the time limit (5 minutes) and size limit (400 megabytes). Zenon uses the tableau method, but it is mention in \cite{bonichon2007zenon} that it isn't that efficient compare to other methods, like resolution.

In 2018, Kusumoto propose deep reinforcement learning algorithm as an automated theorem prover for propositional logic \cite{kusumoto2018automated}. Instead of treating the problem as a planning problem, they treat it as a search problem. Neural Networks are used to figure out which states should be explored while searching. After applying four approximate policy iterations, to allow the algorithm to learn, it solve around 86\% of theorems provide. The main issue with the approach in \cite{kusumoto2018automated} is the need for large amounts of data to train the deep reinforcement learning algorithm.

One of the algorithms we use to generate proofs is resolution. 
There has been plenty of work done to help us implement this algorithm. In fact, Robinson in \cite{robinson1965machine} writes on a theoretical bases on how to resolution while keeping the explanation relevant to computing. In \cite{robinson1965machine}, it is mentioned that first order logic was made to proof algorithms in computers, but resolution allows for simplistic approach using inference instead of deduction. The rest of the paper essentially describes how inference and resolution work, providing many theorems and lemmas. This paper gave us a strong foundation in order to implement resolution.

Bacchus wrote in \cite{bacchus1995using}, that a backwards chaining algorithm has a benefit over forward chaining algorithm in that backwards chaining will never consider actions that aren't relevant to goal, but forward chaining may take require backtracking when a wrong action is taken. On the other hand, a negative of backwards chaining is that it will have less knowledge of the world compare to forward chaining.
backwards chaining will never consider actions that aren't relevant to achieving the goal. The real interesting addition in \cite{bacchus1995using} is that they restrict the domain of the search space by evaluating with first order linear temporal logic. The only differences between first order logic and first order linear temporal logic is that the operation until, always, eventually and next are added. Adding these operations, allows for the domain of the search space of actions to reduce and become more goal driven and solving the downside of forward chaining. The main reason this modification to forward chaining is not applicable to our approach is that in \cite{bacchus1995using} forward chaining was for planning, so they assume that the only positive literals are the goal's set of literals and the rest are negative literals. For generating a valid proof, we can't just assume the goal has the only positive literals and therefor this modification isn't valuable to us.

In \cite{fink1996formalizing}, Fink writes about working both ways. The article talks about having two incomplete plans: head plan and tail plan. The head plan goes from initial state to some current state and gets modified by taking an action from tail plan. The tail plan in just a backward chain from the goal state to the current state. The algorithm in \cite{fink1996formalizing}, named Prodigy, is to make a decision of when to move an action from tail plan to head plan or just backward chain. The general idea of Prodigy is that the algorithm uses the tail plan as a smaller search space to see if the current state can apply any of those actions, if not then the algorithm will chose to backwards chain to grow the search space. This is apparently more efficient than just backwards chaining. The paper brings up the idea of mixing backwards chaining and forward chaining and \cite{fink1996formalizing} describes their solution as backwards chaining with elements of forward chaining.


\bibliographystyle{abbrv}
\bibliography{citations}

\end{document}
